{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","from dotenv import load_dotenv\n","load_dotenv()"]},{"cell_type":"markdown","metadata":{},"source":["# Advanced parsing\n","\n","This method is appropriate if we want more granular control or processing of the page content. Below, instead of generating one Document per page and controlling its content via BeautifulSoup, we generate multiple Document objects representing distinct structures on a page. These structures can include section titles and their corresponding body texts, lists or enumerations, tables, and more.\n","\n","Under the hood it uses the langchain-unstructured library. See the integration docs for more information about using Unstructured with LangChain."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO: NumExpr defaulting to 4 threads.\n"]}],"source":["from langchain_unstructured import UnstructuredLoader\n","\n","page_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"\n","loader = UnstructuredLoader(web_url=page_url)\n","\n","docs = []\n","async for doc in loader.alazy_load():\n","    docs.append(doc)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["66\n"]}],"source":["print(len(docs))"]},{"cell_type":"markdown","metadata":{},"source":["Note that with no advance knowledge of the page HTML structure, we recover a natural organization of the body text:\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["How to add memory to chatbots\n","A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:\n","Simply stuffing previous messages into a chat model prompt.\n","The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n","More complex modifications like synthesizing summaries for long running conversations.\n","We'll go into more detail on a few techniques below!\n","note\n","This how-to guide previously built a chatbot using RunnableWithMessageHistory. You can access this version of the guide in the v0.2 docs.\n","As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.\n","If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.\n","Please see How to migrate to LangGraph Memory for more details.\n","Setup​\n","You'll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\n","%pip install --upgrade --quiet langchain langchain-openai langgraph\n","\n","import getpass\n","import os\n","\n","if not os.environ.get(\"OPENAI_API_KEY\"):\n","    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n","OpenAI API Key: ········\n","Let's also set up a chat model that we'll use for the below examples.\n","from langchain_openai import ChatOpenAI\n","\n","model = ChatOpenAI(model=\"gpt-4o-mini\")\n","API Reference:ChatOpenAI\n","Message passing​\n","The simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n","from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        SystemMessage(\n","            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n","        ),\n","        MessagesPlaceholder(variable_name=\"messages\"),\n","    ]\n",")\n","\n","chain = prompt | model\n","\n","ai_msg = chain.invoke(\n","    {\n","        \"messages\": [\n","            HumanMessage(\n","                content=\"Translate from English to French: I love programming.\"\n","            ),\n","            AIMessage(content=\"J'adore la programmation.\"),\n","            HumanMessage(content=\"What did you just say?\"),\n","        ],\n","    }\n",")\n","print(ai_msg.content)\n","API Reference:AIMessage | HumanMessage | SystemMessage | ChatPromptTemplate | MessagesPlaceholder\n","I said, \"I love programming\" in French: \"J'adore la programmation.\"\n","We can see that by passing the previous conversation into a chain, it can use it as context to answer questions. This is the basic concept underpinning chatbot memory - the rest of the guide will demonstrate convenient techniques for passing or reformatting messages.\n","Automatic history management​\n","The previous examples pass messages to the chain (and model) explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also provides a way to build applications that have memory using LangGraph's persistence. You can enable persistence in LangGraph applications by providing a checkpointer when compiling the graph.\n","from langgraph.checkpoint.memory import MemorySaver\n","from langgraph.graph import START, MessagesState, StateGraph\n","\n","workflow = StateGraph(state_schema=MessagesState)\n","\n","\n","# Define the function that calls the model\n","def call_model(state: MessagesState):\n","    system_prompt = (\n","        \"You are a helpful assistant. \"\n","        \"Answer all questions to the best of your ability.\"\n","    )\n","    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n","    response = model.invoke(messages)\n","    return {\"messages\": response}\n","\n","\n","# Define the node and edge\n","workflow.add_node(\"model\", call_model)\n","workflow.add_edge(START, \"model\")\n","\n","# Add simple in-memory checkpointer\n","memory = MemorySaver()\n","app = workflow.compile(checkpointer=memory)\n","API Reference:MemorySaver | StateGraph\n","We'll pass the latest input to the conversation here and let the LangGraph keep track of the conversation history using the checkpointer:\n","app.invoke(\n","    {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]},\n","    config={\"configurable\": {\"thread_id\": \"1\"}},\n",")\n","{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),\n","  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39})]}\n","app.invoke(\n","    {\"messages\": [HumanMessage(content=\"What did I just ask you?\")]},\n","    config={\"configurable\": {\"thread_id\": \"1\"}},\n",")\n","{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),\n","  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39}),\n","  HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='c667529b-7c41-4cc0-9326-0af47328b816'),\n","  AIMessage(content='You asked me to translate \"I love programming\" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 54, 'total_tokens': 67, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-134a7ea0-d3a4-4923-bd58-25e5a43f6a1f-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67})]}\n","Modifying chat history​\n","Modifying stored chat messages can help your chatbot handle a variety of situations. Here are some examples:\n","Trimming messages​\n","LLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is trim the history messages before passing them to the model. Let's use an example history with the app we declared above:\n","demo_ephemeral_chat_history = [\n","    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n","    AIMessage(content=\"Hello!\"),\n","    HumanMessage(content=\"How are you today?\"),\n","    AIMessage(content=\"Fine thanks!\"),\n","]\n","\n","app.invoke(\n","    {\n","        \"messages\": demo_ephemeral_chat_history\n","        + [HumanMessage(content=\"What's my name?\")]\n","    },\n","    config={\"configurable\": {\"thread_id\": \"2\"}},\n",")\n","{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),\n","  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),\n","  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),\n","  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),\n","  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='c933eca3-5fd8-4651-af16-20fe2d49c216'),\n","  AIMessage(content='Your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 63, 'total_tokens': 68, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a0b21acc-9dbb-4fb6-a953-392020f37d88-0', usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68})]}\n","We can see the app remembers the preloaded name.\n","But let's say we have a very small context window, and we want to trim the number of messages passed to the model to only the 2 most recent ones. We can use the built in trim_messages util to trim messages based on their token count before they reach our prompt. In this case we'll count each message as 1 \"token\" and keep only the last two messages:\n","from langchain_core.messages import trim_messages\n","from langgraph.checkpoint.memory import MemorySaver\n","from langgraph.graph import START, MessagesState, StateGraph\n","\n","# Define trimmer\n","# count each message as 1 \"token\" (token_counter=len) and keep only the last two messages\n","trimmer = trim_messages(strategy=\"last\", max_tokens=2, token_counter=len)\n","\n","workflow = StateGraph(state_schema=MessagesState)\n","\n","\n","# Define the function that calls the model\n","def call_model(state: MessagesState):\n","    trimmed_messages = trimmer.invoke(state[\"messages\"])\n","    system_prompt = (\n","        \"You are a helpful assistant. \"\n","        \"Answer all questions to the best of your ability.\"\n","    )\n","    messages = [SystemMessage(content=system_prompt)] + trimmed_messages\n","    response = model.invoke(messages)\n","    return {\"messages\": response}\n","\n","\n","# Define the node and edge\n","workflow.add_node(\"model\", call_model)\n","workflow.add_edge(START, \"model\")\n","\n","# Add simple in-memory checkpointer\n","memory = MemorySaver()\n","app = workflow.compile(checkpointer=memory)\n","API Reference:trim_messages | MemorySaver | StateGraph\n","Let's call this new app and check the response\n","app.invoke(\n","    {\n","        \"messages\": demo_ephemeral_chat_history\n","        + [HumanMessage(content=\"What is my name?\")]\n","    },\n","    config={\"configurable\": {\"thread_id\": \"3\"}},\n",")\n","{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),\n","  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),\n","  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),\n","  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),\n","  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='a22ab7c5-8617-4821-b3e9-a9e7dca1ff78'),\n","  AIMessage(content=\"I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 39, 'total_tokens': 66, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7b32d72-9f57-4705-be7e-43bf1c3d293b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 27, 'total_tokens': 66})]}\n","We can see that trim_messages was called and only the two most recent messages will be passed to the model. In this case, this means that the model forgot the name we gave it.\n","Check out our how to guide on trimming messages for more.\n","Summary memory​\n","We can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our app. Let's recreate our chat history:\n","demo_ephemeral_chat_history = [\n","    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n","    AIMessage(content=\"Hello!\"),\n","    HumanMessage(content=\"How are you today?\"),\n","    AIMessage(content=\"Fine thanks!\"),\n","]\n","And now, let's update the model-calling function to distill previous interactions into a summary:\n","from langchain_core.messages import HumanMessage, RemoveMessage\n","from langgraph.checkpoint.memory import MemorySaver\n","from langgraph.graph import START, MessagesState, StateGraph\n","\n","workflow = StateGraph(state_schema=MessagesState)\n","\n","\n","# Define the function that calls the model\n","def call_model(state: MessagesState):\n","    system_prompt = (\n","        \"You are a helpful assistant. \"\n","        \"Answer all questions to the best of your ability. \"\n","        \"The provided chat history includes a summary of the earlier conversation.\"\n","    )\n","    system_message = SystemMessage(content=system_prompt)\n","    message_history = state[\"messages\"][:-1]  # exclude the most recent user input\n","    # Summarize the messages if the chat history reaches a certain size\n","    if len(message_history) >= 4:\n","        last_human_message = state[\"messages\"][-1]\n","        # Invoke the model to generate conversation summary\n","        summary_prompt = (\n","            \"Distill the above chat messages into a single summary message. \"\n","            \"Include as many specific details as you can.\"\n","        )\n","        summary_message = model.invoke(\n","            message_history + [HumanMessage(content=summary_prompt)]\n","        )\n","\n","        # Delete messages that we no longer want to show up\n","        delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"]]\n","        # Re-add user message\n","        human_message = HumanMessage(content=last_human_message.content)\n","        # Call the model with summary & response\n","        response = model.invoke([system_message, summary_message, human_message])\n","        message_updates = [summary_message, human_message, response] + delete_messages\n","    else:\n","        message_updates = model.invoke([system_message] + state[\"messages\"])\n","\n","    return {\"messages\": message_updates}\n","\n","\n","# Define the node and edge\n","workflow.add_node(\"model\", call_model)\n","workflow.add_edge(START, \"model\")\n","\n","# Add simple in-memory checkpointer\n","memory = MemorySaver()\n","app = workflow.compile(checkpointer=memory)\n","API Reference:HumanMessage | RemoveMessage | MemorySaver | StateGraph\n","Let's see if it remembers the name we gave it:\n","app.invoke(\n","    {\n","        \"messages\": demo_ephemeral_chat_history\n","        + [HumanMessage(\"What did I say my name was?\")]\n","    },\n","    config={\"configurable\": {\"thread_id\": \"4\"}},\n",")\n","{'messages': [AIMessage(content=\"Nemo greeted me, and I responded positively, indicating that I'm doing well.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 60, 'total_tokens': 76, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee42f98d-907d-4bad-8f16-af2db789701d-0', usage_metadata={'input_tokens': 60, 'output_tokens': 16, 'total_tokens': 76}),\n","  HumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={}, id='788555ea-5b1f-4c29-a2f2-a92f15d147be'),\n","  AIMessage(content='You mentioned that your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 67, 'total_tokens': 75, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-099a43bd-a284-4969-bb6f-0be486614cd8-0', usage_metadata={'input_tokens': 67, 'output_tokens': 8, 'total_tokens': 75})]}\n","Note that invoking the app again will keep accumulating the history until it reaches the specified number of messages (four in our case). At that point we will generate another summary generated from the initial summary plus new messages and so on.\n","Edit this page\n","Was this page helpful?\n","Setup\n","Message passing\n","Automatic history management\n","Modifying chat history\n","Trimming messages\n","Summary memory\n"]}],"source":["for doc in docs:\n","    print(doc.page_content)"]},{"cell_type":"markdown","metadata":{},"source":["# Extracting content from specific sections\n","\n","Each ```Document``` object represents an element of the page. Its metadata contains useful information, such as its category:"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'source': 'https://python.langchain.com/docs/how_to/chatbots_memory/'}\n","\n","How to add memory to chatbots | A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including: | Simply stuffing previous messages into a chat model prompt. | The above, but trimming old messages to reduce the amount of distracting information the model has to deal with. | More complex modifications like synthesizing summaries for long running conversations. | We'll go into more detail on a few techniques below! | note | This how-to guide previously built a chatbot using | RunnableWithMessageHistory | . You can access this version of the guide in the | v0.2 docs | . | As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of | LangGraph persistence | to incorporate | memory | into new LangChain applications. | If your code is already relying on | RunnableWithMessageHistory | or | BaseChatMessageHistory | , you do | not | need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses | RunnableWithMessageHistory | will continue to work as expected. | Please see | How to migrate to LangGraph Memory | for more details. | Setup | ‚Äã | You'll need to install a few packages, and have your OpenAI API key set as an environment variable named | OPENAI_API_KEY | : | % | pip install | - | - | upgrade | - | - | quiet langchain langchain | - | openai langgraph | import | getpass | import | os | if | not | os | . | environ | . | get | ( | \"OPENAI_API_KEY\" | ) | : | os | . | environ | [ | \"OPENAI_API_KEY\" | ] | = | getpass | . | getpass | ( | \"OpenAI API Key:\" | ) | OpenAI API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑ | Let's also set up a chat model that we'll use for the below examples. | from | langchain_openai | import | ChatOpenAI | model | = | ChatOpenAI | ( | model | = | \"gpt-4o-mini\" | ) | API Reference: | ChatOpenAI | Message passing | ‚Äã | The simplest form of memory is simply passing chat history messages into a chain. Here's an example: | from | langchain_core | . | messages | import | AIMessage | , | HumanMessage | , | SystemMessage | from | langchain_core | . | prompts | import | ChatPromptTemplate | , | MessagesPlaceholder | prompt | = | ChatPromptTemplate | . | from_messages | ( | [ | SystemMessage | ( | content | = | \"You are a helpful assistant. Answer all questions to the best of your ability.\" | ) | , | MessagesPlaceholder | ( | variable_name | = | \"messages\" | ) | , | ] | ) | chain | = | prompt | | | model | ai_msg | = | chain | . | invoke | ( | { | \"messages\" | : | [ | HumanMessage | ( | content | = | \"Translate from English to French: I love programming.\" | ) | , | AIMessage | ( | content | = | \"J'adore la programmation.\" | ) | , | HumanMessage | ( | content | = | \"What did you just say?\" | ) | , | ] | , | } | ) | print | ( | ai_msg | . | content | ) | API Reference: | AIMessage | | | HumanMessage | | | SystemMessage | | | ChatPromptTemplate | | | MessagesPlaceholder | I said, \"I love programming\" in French: \"J'adore la programmation.\" | We can see that by passing the previous conversation into a chain, it can use it as context to answer questions. This is the basic concept underpinning chatbot memory - the rest of the guide will demonstrate convenient techniques for passing or reformatting messages. | Automatic history management | ‚Äã | The previous examples pass messages to the chain (and model) explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also provides a way to build applications that have memory using LangGraph's | persistence | . You can | enable persistence | in LangGraph applications by providing a | checkpointer | when compiling the graph. | from | langgraph | . | checkpoint | . | memory | import | MemorySaver | from | langgraph | . | graph | import | START | , | MessagesState | , | StateGraph | workflow | = | StateGraph | ( | state_schema | = | MessagesState | ) | # Define the function that calls the model | def | call_model | ( | state | : | MessagesState | ) | : | system_prompt | = | ( | \"You are a helpful assistant. \" | \"Answer all questions to the best of your ability.\" | ) | messages | = | [ | SystemMessage | ( | content | = | system_prompt | ) | ] | + | state | [ | \"messages\" | ] | response | = | model | . | invoke | ( | messages | ) | return | { | \"messages\" | : | response | } | # Define the node and edge | workflow | . | add_node | ( | \"model\" | , | call_model | ) | workflow | . | add_edge | ( | START | , | \"model\" | ) | # Add simple in-memory checkpointer | memory | = | MemorySaver | ( | ) | app | = | workflow | . | compile | ( | checkpointer | = | memory | ) | API Reference: | MemorySaver | | | StateGraph | We'll pass the latest input to the conversation here and let the LangGraph keep track of the conversation history using the checkpointer: | app | . | invoke | ( | { | \"messages\" | : | [ | HumanMessage | ( | content | = | \"Translate to French: I love programming.\" | ) | ] | } | , | config | = | { | \"configurable\" | : | { | \"thread_id\" | : | \"1\" | } | } | , | ) | {'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'), | AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39})]} | app | . | invoke | ( | { | \"messages\" | : | [ | HumanMessage | ( | content | = | \"What did I just ask you?\" | ) | ] | } | , | config | = | { | \"configurable\" | : | { | \"thread_id\" | : | \"1\" | } | } | , | ) | {'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'), | AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39}), | HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='c667529b-7c41-4cc0-9326-0af47328b816'), | AIMessage(content='You asked me to translate \"I love programming\" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 54, 'total_tokens': 67, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-134a7ea0-d3a4-4923-bd58-25e5a43f6a1f-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67})]} | Modifying chat history | ‚Äã | Modifying stored chat messages can help your chatbot handle a variety of situations. Here are some examples: | Trimming messages | ‚Äã | LLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is trim the history messages before passing them to the model. Let's use an example history with the | app | we declared above: | demo_ephemeral_chat_history | = | [ | HumanMessage | ( | content | = | \"Hey there! I'm Nemo.\" | ) | , | AIMessage | ( | content | = | \"Hello!\" | ) | , | HumanMessage | ( | content | = | \"How are you today?\" | ) | , | AIMessage | ( | content | = | \"Fine thanks!\" | ) | , | ] | app | . | invoke | ( | { | \"messages\" | : | demo_ephemeral_chat_history | + | [ | HumanMessage | ( | content | = | \"What's my name?\" | ) | ] | } | , | config | = | { | \"configurable\" | : | { | \"thread_id\" | : | \"2\" | } | } | , | ) | {'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'), | AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'), | HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'), | AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'), | HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='c933eca3-5fd8-4651-af16-20fe2d49c216'), | AIMessage(content='Your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 63, 'total_tokens': 68, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a0b21acc-9dbb-4fb6-a953-392020f37d88-0', usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68})]} | We can see the app remembers the preloaded name. | But let's say we have a very small context window, and we want to trim the number of messages passed to the model to only the 2 most recent ones. We can use the built in | trim_messages | util to trim messages based on their token count before they reach our prompt. In this case we'll count each message as 1 \"token\" and keep only the last two messages: | from | langchain_core | . | messages | import | trim_messages | from | langgraph | . | checkpoint | . | memory | import | MemorySaver | from | langgraph | . | graph | import | START | , | MessagesState | , | StateGraph | # Define trimmer | # count each message as 1 \"token\" (token_counter=len) and keep only the last two messages | trimmer | = | trim_messages | ( | strategy | = | \"last\" | , | max_tokens | = | 2 | , | token_counter | = | len | ) | workflow | = | StateGraph | ( | state_schema | = | MessagesState | ) | # Define the function that calls the model | def | call_model | ( | state | : | MessagesState | ) | : | trimmed_messages | = | trimmer | . | invoke | ( | state | [ | \"messages\" | ] | ) | system_prompt | = | ( | \"You are a helpful assistant. \" | \"Answer all questions to the best of your ability.\" | ) | messages | = | [ | SystemMessage | ( | content | = | system_prompt | ) | ] | + | trimmed_messages | response | = | model | . | invoke | ( | messages | ) | return | { | \"messages\" | : | response | } | # Define the node and edge | workflow | . | add_node | ( | \"model\" | , | call_model | ) | workflow | . | add_edge | ( | START | , | \"model\" | ) | # Add simple in-memory checkpointer | memory | = | MemorySaver | ( | ) | app | = | workflow | . | compile | ( | checkpointer | = | memory | ) | API Reference: | trim_messages | | | MemorySaver | | | StateGraph | Let's call this new app and check the response | app | . | invoke | ( | { | \"messages\" | : | demo_ephemeral_chat_history | + | [ | HumanMessage | ( | content | = | \"What is my name?\" | ) | ] | } | , | config | = | { | \"configurable\" | : | { | \"thread_id\" | : | \"3\" | } | } | , | ) | {'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'), | AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'), | HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'), | AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'), | HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='a22ab7c5-8617-4821-b3e9-a9e7dca1ff78'), | AIMessage(content=\"I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 39, 'total_tokens': 66, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7b32d72-9f57-4705-be7e-43bf1c3d293b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 27, 'total_tokens': 66})]} | We can see that | trim_messages | was called and only the two most recent messages will be passed to the model. In this case, this means that the model forgot the name we gave it. | Check out our | how to guide on trimming messages | for more. | Summary memory | ‚Äã | We can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our app. Let's recreate our chat history: | demo_ephemeral_chat_history | = | [ | HumanMessage | ( | content | = | \"Hey there! I'm Nemo.\" | ) | , | AIMessage | ( | content | = | \"Hello!\" | ) | , | HumanMessage | ( | content | = | \"How are you today?\" | ) | , | AIMessage | ( | content | = | \"Fine thanks!\" | ) | , | ] | And now, let's update the model-calling function to distill previous interactions into a summary: | from | langchain_core | . | messages | import | HumanMessage | , | RemoveMessage | from | langgraph | . | checkpoint | . | memory | import | MemorySaver | from | langgraph | . | graph | import | START | , | MessagesState | , | StateGraph | workflow | = | StateGraph | ( | state_schema | = | MessagesState | ) | # Define the function that calls the model | def | call_model | ( | state | : | MessagesState | ) | : | system_prompt | = | ( | \"You are a helpful assistant. \" | \"Answer all questions to the best of your ability. \" | \"The provided chat history includes a summary of the earlier conversation.\" | ) | system_message | = | SystemMessage | ( | content | = | system_prompt | ) | message_history | = | state | [ | \"messages\" | ] | [ | : | - | 1 | ] | # exclude the most recent user input | # Summarize the messages if the chat history reaches a certain size | if | len | ( | message_history | ) | >= | 4 | : | last_human_message | = | state | [ | \"messages\" | ] | [ | - | 1 | ] | # Invoke the model to generate conversation summary | summary_prompt | = | ( | \"Distill the above chat messages into a single summary message. \" | \"Include as many specific details as you can.\" | ) | summary_message | = | model | . | invoke | ( | message_history | + | [ | HumanMessage | ( | content | = | summary_prompt | ) | ] | ) | # Delete messages that we no longer want to show up | delete_messages | = | [ | RemoveMessage | ( | id | = | m | . | id | ) | for | m | in | state | [ | \"messages\" | ] | ] | # Re-add user message | human_message | = | HumanMessage | ( | content | = | last_human_message | . | content | ) | # Call the model with summary & response | response | = | model | . | invoke | ( | [ | system_message | , | summary_message | , | human_message | ] | ) | message_updates | = | [ | summary_message | , | human_message | , | response | ] | + | delete_messages | else | : | message_updates | = | model | . | invoke | ( | [ | system_message | ] | + | state | [ | \"messages\" | ] | ) | return | { | \"messages\" | : | message_updates | } | # Define the node and edge | workflow | . | add_node | ( | \"model\" | , | call_model | ) | workflow | . | add_edge | ( | START | , | \"model\" | ) | # Add simple in-memory checkpointer | memory | = | MemorySaver | ( | ) | app | = | workflow | . | compile | ( | checkpointer | = | memory | ) | API Reference: | HumanMessage | | | RemoveMessage | | | MemorySaver | | | StateGraph | Let's see if it remembers the name we gave it: | app | . | invoke | ( | { | \"messages\" | : | demo_ephemeral_chat_history | + | [ | HumanMessage | ( | \"What did I say my name was?\" | ) | ] | } | , | config | = | { | \"configurable\" | : | { | \"thread_id\" | : | \"4\" | } | } | , | ) | {'messages': [AIMessage(content=\"Nemo greeted me, and I responded positively, indicating that I'm doing well.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 60, 'total_tokens': 76, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee42f98d-907d-4bad-8f16-af2db789701d-0', usage_metadata={'input_tokens': 60, 'output_tokens': 16, 'total_tokens': 76}), | HumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={}, id='788555ea-5b1f-4c29-a2f2-a92f15d147be'), | AIMessage(content='You mentioned that your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 67, 'total_tokens': 75, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-099a43bd-a284-4969-bb6f-0be486614cd8-0', usage_metadata={'input_tokens': 67, 'output_tokens': 8, 'total_tokens': 75})]} | Note that invoking the app again will keep accumulating the history until it reaches the specified number of messages (four in our case). At that point we will generate another summary generated from the initial summary plus new messages and so on.\n"]}],"source":["print(f\"{doc.metadata}\\n\")\n","print(doc.page_content)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Title: How to add memory to chatbots\n","NarrativeText: A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:\n","ListItem: Simply stuffing previous messages into a chat model prompt.\n","ListItem: The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n","ListItem: More complex modifications like synthesizing summaries for long running conversations.\n","NarrativeText: We'll go into more detail on a few techniques below!\n","Title: note\n","NarrativeText: This how-to guide previously built a chatbot using RunnableWithMessageHistory. You can access this version of the guide in the v0.2 docs.\n","NarrativeText: As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.\n","NarrativeText: If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.\n","NarrativeText: Please see How to migrate to LangGraph Memory for more details.\n","Title: Setup​\n","NarrativeText: You'll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\n","NarrativeText: %pip install --upgrade --quiet langchain langchain-openai langgraph\n","\n","import getpass\n","import os\n","\n","if not os.environ.get(\"OPENAI_API_KEY\"):\n","    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n","Title: OpenAI API Key: ········\n","NarrativeText: Let's also set up a chat model that we'll use for the below examples.\n","Title: from langchain_openai import ChatOpenAI\n","\n","model = ChatOpenAI(model=\"gpt-4o-mini\")\n","Title: API Reference:ChatOpenAI\n","Title: Message passing​\n","NarrativeText: The simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n","NarrativeText: from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        SystemMessage(\n","            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n","        ),\n","        MessagesPlaceholder(variable_name=\"messages\"),\n","    ]\n",")\n","\n","chain = prompt | model\n","\n","ai_msg = chain.invoke(\n","    {\n","        \"messages\": [\n","            HumanMessage(\n","                content=\"Translate from English to French: I love programming.\"\n","            ),\n","            AIMessage(content=\"J'adore la programmation.\"),\n","            HumanMessage(content=\"What did you just say?\"),\n","        ],\n","    }\n",")\n","print(ai_msg.content)\n","Title: API Reference:AIMessage | HumanMessage | SystemMessage | ChatPromptTemplate | MessagesPlaceholder\n","NarrativeText: I said, \"I love programming\" in French: \"J'adore la programmation.\"\n","NarrativeText: We can see that by passing the previous conversation into a chain, it can use it as context to answer questions. This is the basic concept underpinning chatbot memory - the rest of the guide will demonstrate convenient techniques for passing or reformatting messages.\n","Title: Automatic history management​\n","NarrativeText: The previous examples pass messages to the chain (and model) explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also provides a way to build applications that have memory using LangGraph's persistence. You can enable persistence in LangGraph applications by providing a checkpointer when compiling the graph.\n","NarrativeText: from langgraph.checkpoint.memory import MemorySaver\n","from langgraph.graph import START, MessagesState, StateGraph\n","\n","workflow = StateGraph(state_schema=MessagesState)\n","\n","\n","# Define the function that calls the model\n","def call_model(state: MessagesState):\n","    system_prompt = (\n","        \"You are a helpful assistant. \"\n","        \"Answer all questions to the best of your ability.\"\n","    )\n","    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n","    response = model.invoke(messages)\n","    return {\"messages\": response}\n","\n","\n","# Define the node and edge\n","workflow.add_node(\"model\", call_model)\n","workflow.add_edge(START, \"model\")\n","\n","# Add simple in-memory checkpointer\n","memory = MemorySaver()\n","app = workflow.compile(checkpointer=memory)\n","Title: API Reference:MemorySaver | StateGraph\n","NarrativeText: We'll pass the latest input to the conversation here and let the LangGraph keep track of the conversation history using the checkpointer:\n","NarrativeText: app.invoke(\n","    {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]},\n","    config={\"configurable\": {\"thread_id\": \"1\"}},\n",")\n","NarrativeText: {'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),\n","  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39})]}\n","NarrativeText: app.invoke(\n","    {\"messages\": [HumanMessage(content=\"What did I just ask you?\")]},\n","    config={\"configurable\": {\"thread_id\": \"1\"}},\n",")\n","NarrativeText: {'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),\n","  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39}),\n","  HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='c667529b-7c41-4cc0-9326-0af47328b816'),\n","  AIMessage(content='You asked me to translate \"I love programming\" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 54, 'total_tokens': 67, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-134a7ea0-d3a4-4923-bd58-25e5a43f6a1f-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67})]}\n","Title: Modifying chat history​\n","NarrativeText: Modifying stored chat messages can help your chatbot handle a variety of situations. Here are some examples:\n","Title: Trimming messages​\n","NarrativeText: LLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is trim the history messages before passing them to the model. Let's use an example history with the app we declared above:\n","NarrativeText: demo_ephemeral_chat_history = [\n","    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n","    AIMessage(content=\"Hello!\"),\n","    HumanMessage(content=\"How are you today?\"),\n","    AIMessage(content=\"Fine thanks!\"),\n","]\n","\n","app.invoke(\n","    {\n","        \"messages\": demo_ephemeral_chat_history\n","        + [HumanMessage(content=\"What's my name?\")]\n","    },\n","    config={\"configurable\": {\"thread_id\": \"2\"}},\n",")\n","NarrativeText: {'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),\n","  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),\n","  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),\n","  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),\n","  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='c933eca3-5fd8-4651-af16-20fe2d49c216'),\n","  AIMessage(content='Your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 63, 'total_tokens': 68, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a0b21acc-9dbb-4fb6-a953-392020f37d88-0', usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68})]}\n","NarrativeText: We can see the app remembers the preloaded name.\n","NarrativeText: But let's say we have a very small context window, and we want to trim the number of messages passed to the model to only the 2 most recent ones. We can use the built in trim_messages util to trim messages based on their token count before they reach our prompt. In this case we'll count each message as 1 \"token\" and keep only the last two messages:\n","NarrativeText: from langchain_core.messages import trim_messages\n","from langgraph.checkpoint.memory import MemorySaver\n","from langgraph.graph import START, MessagesState, StateGraph\n","\n","# Define trimmer\n","# count each message as 1 \"token\" (token_counter=len) and keep only the last two messages\n","trimmer = trim_messages(strategy=\"last\", max_tokens=2, token_counter=len)\n","\n","workflow = StateGraph(state_schema=MessagesState)\n","\n","\n","# Define the function that calls the model\n","def call_model(state: MessagesState):\n","    trimmed_messages = trimmer.invoke(state[\"messages\"])\n","    system_prompt = (\n","        \"You are a helpful assistant. \"\n","        \"Answer all questions to the best of your ability.\"\n","    )\n","    messages = [SystemMessage(content=system_prompt)] + trimmed_messages\n","    response = model.invoke(messages)\n","    return {\"messages\": response}\n","\n","\n","# Define the node and edge\n","workflow.add_node(\"model\", call_model)\n","workflow.add_edge(START, \"model\")\n","\n","# Add simple in-memory checkpointer\n","memory = MemorySaver()\n","app = workflow.compile(checkpointer=memory)\n","NarrativeText: API Reference:trim_messages | MemorySaver | StateGraph\n","NarrativeText: Let's call this new app and check the response\n","NarrativeText: app.invoke(\n","    {\n","        \"messages\": demo_ephemeral_chat_history\n","        + [HumanMessage(content=\"What is my name?\")]\n","    },\n","    config={\"configurable\": {\"thread_id\": \"3\"}},\n",")\n","NarrativeText: {'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),\n","  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),\n","  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),\n","  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),\n","  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='a22ab7c5-8617-4821-b3e9-a9e7dca1ff78'),\n","  AIMessage(content=\"I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 39, 'total_tokens': 66, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7b32d72-9f57-4705-be7e-43bf1c3d293b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 27, 'total_tokens': 66})]}\n","NarrativeText: We can see that trim_messages was called and only the two most recent messages will be passed to the model. In this case, this means that the model forgot the name we gave it.\n","NarrativeText: Check out our how to guide on trimming messages for more.\n","Title: Summary memory​\n","NarrativeText: We can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our app. Let's recreate our chat history:\n","NarrativeText: demo_ephemeral_chat_history = [\n","    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n","    AIMessage(content=\"Hello!\"),\n","    HumanMessage(content=\"How are you today?\"),\n","    AIMessage(content=\"Fine thanks!\"),\n","]\n","NarrativeText: And now, let's update the model-calling function to distill previous interactions into a summary:\n","NarrativeText: from langchain_core.messages import HumanMessage, RemoveMessage\n","from langgraph.checkpoint.memory import MemorySaver\n","from langgraph.graph import START, MessagesState, StateGraph\n","\n","workflow = StateGraph(state_schema=MessagesState)\n","\n","\n","# Define the function that calls the model\n","def call_model(state: MessagesState):\n","    system_prompt = (\n","        \"You are a helpful assistant. \"\n","        \"Answer all questions to the best of your ability. \"\n","        \"The provided chat history includes a summary of the earlier conversation.\"\n","    )\n","    system_message = SystemMessage(content=system_prompt)\n","    message_history = state[\"messages\"][:-1]  # exclude the most recent user input\n","    # Summarize the messages if the chat history reaches a certain size\n","    if len(message_history) >= 4:\n","        last_human_message = state[\"messages\"][-1]\n","        # Invoke the model to generate conversation summary\n","        summary_prompt = (\n","            \"Distill the above chat messages into a single summary message. \"\n","            \"Include as many specific details as you can.\"\n","        )\n","        summary_message = model.invoke(\n","            message_history + [HumanMessage(content=summary_prompt)]\n","        )\n","\n","        # Delete messages that we no longer want to show up\n","        delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"]]\n","        # Re-add user message\n","        human_message = HumanMessage(content=last_human_message.content)\n","        # Call the model with summary & response\n","        response = model.invoke([system_message, summary_message, human_message])\n","        message_updates = [summary_message, human_message, response] + delete_messages\n","    else:\n","        message_updates = model.invoke([system_message] + state[\"messages\"])\n","\n","    return {\"messages\": message_updates}\n","\n","\n","# Define the node and edge\n","workflow.add_node(\"model\", call_model)\n","workflow.add_edge(START, \"model\")\n","\n","# Add simple in-memory checkpointer\n","memory = MemorySaver()\n","app = workflow.compile(checkpointer=memory)\n","Title: API Reference:HumanMessage | RemoveMessage | MemorySaver | StateGraph\n","NarrativeText: Let's see if it remembers the name we gave it:\n","NarrativeText: app.invoke(\n","    {\n","        \"messages\": demo_ephemeral_chat_history\n","        + [HumanMessage(\"What did I say my name was?\")]\n","    },\n","    config={\"configurable\": {\"thread_id\": \"4\"}},\n",")\n","NarrativeText: {'messages': [AIMessage(content=\"Nemo greeted me, and I responded positively, indicating that I'm doing well.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 60, 'total_tokens': 76, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee42f98d-907d-4bad-8f16-af2db789701d-0', usage_metadata={'input_tokens': 60, 'output_tokens': 16, 'total_tokens': 76}),\n","  HumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={}, id='788555ea-5b1f-4c29-a2f2-a92f15d147be'),\n","  AIMessage(content='You mentioned that your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 67, 'total_tokens': 75, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-099a43bd-a284-4969-bb6f-0be486614cd8-0', usage_metadata={'input_tokens': 67, 'output_tokens': 8, 'total_tokens': 75})]}\n","NarrativeText: Note that invoking the app again will keep accumulating the history until it reaches the specified number of messages (four in our case). At that point we will generate another summary generated from the initial summary plus new messages and so on.\n","Title: Edit this page\n","Title: Was this page helpful?\n","ListItem: Setup\n","ListItem: Message passing\n","ListItem: Automatic history management\n","ListItem: Modifying chat history\n","ListItem: Trimming messages\n","ListItem: Summary memory\n"]}],"source":["for doc in docs:\n","    print(f'{doc.metadata[\"category\"]}: {doc.page_content}')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"application/json":{"category":"Title","category_depth":0,"element_id":"fe037635255efdcb12c13437486c388b","filetype":"text/html","languages":["eng"],"url":"https://python.langchain.com/docs/how_to/chatbots_memory/"},"text/plain":["<IPython.core.display.JSON object>"]},"metadata":{"application/json":{"expanded":false,"root":"root"}},"output_type":"display_data"}],"source":["from IPython.display import display, JSON\n","display(JSON(docs[0].metadata))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'category_depth': 0, 'languages': ['eng'], 'filetype': 'text/html', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': 'fe037635255efdcb12c13437486c388b'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'fe037635255efdcb12c13437486c388b', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '6d776151467bd8f096fc404de3ad502a'}\n","{'category_depth': 1, 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'fe037635255efdcb12c13437486c388b', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'ListItem', 'element_id': '465d3d600e2200c5575addbe51297f9d'}\n","{'category_depth': 1, 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'fe037635255efdcb12c13437486c388b', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'ListItem', 'element_id': '7215434059e03b088853eda85b628e98'}\n","{'category_depth': 1, 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'fe037635255efdcb12c13437486c388b', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'ListItem', 'element_id': '9cc5d54de67370ba3328450709f52659'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'fe037635255efdcb12c13437486c388b', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '89bbfbe7e5e66d63b93797fb7a2100aa'}\n","{'category_depth': 0, 'languages': ['eng'], 'filetype': 'text/html', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': '890a2bf3144778d4f61efefaed21b5b4'}\n","{'link_texts': ['RunnableWithMessageHistory', 'v0.2 docs'], 'link_urls': ['https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html', 'https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '890a2bf3144778d4f61efefaed21b5b4', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '827195df3a5f2e05fadc1ac24a27a052'}\n","{'link_texts': ['LangGraph persistence'], 'link_urls': ['https://langchain-ai.github.io/langgraph/concepts/persistence/'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '890a2bf3144778d4f61efefaed21b5b4', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'e27a0d7f1c92ed839eec537134095b15'}\n","{'emphasized_text_contents': ['not'], 'emphasized_text_tags': ['b'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '890a2bf3144778d4f61efefaed21b5b4', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '286a2576d98ed8eac366ada1aedd9f59'}\n","{'link_texts': ['How to migrate to LangGraph Memory'], 'link_urls': ['/docs/versions/migrating_memory/'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '890a2bf3144778d4f61efefaed21b5b4', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'b64dacdaf28f5b59fe0c068515abe8ab'}\n","{'category_depth': 1, 'link_texts': ['\\u200b'], 'link_urls': ['#setup'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '890a2bf3144778d4f61efefaed21b5b4', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': '3594ca77394243c69b8af10de036a119'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '3594ca77394243c69b8af10de036a119', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'ff81661a5006d789d756bcb44a227f37'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '3594ca77394243c69b8af10de036a119', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '9e6c966a97f14438bf50847978a17b6e'}\n","{'category_depth': 0, 'languages': ['eng'], 'filetype': 'text/html', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': 'ebb238ad1dc52a862cccfc3007b80a14'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'ebb238ad1dc52a862cccfc3007b80a14', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '17fef4b0114b83273b0c9ba7fcf34ea5'}\n","{'category_depth': 0, 'languages': ['eng'], 'filetype': 'text/html', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': '9e3ceeb48342dddb634e18ff05b93cd5'}\n","{'category_depth': 0, 'emphasized_text_contents': ['API Reference:'], 'emphasized_text_tags': ['b'], 'link_texts': ['ChatOpenAI'], 'link_urls': ['https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html'], 'languages': ['eng'], 'filetype': 'text/html', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': '9544719e26a56001be566ebf3a48cbb7'}\n","{'category_depth': 1, 'link_texts': ['\\u200b'], 'link_urls': ['#message-passing'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '9544719e26a56001be566ebf3a48cbb7', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': 'ca2da8fef6b0e723738eb88852f82391'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'ca2da8fef6b0e723738eb88852f82391', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '97c55afdcee1be4577d639fc89f1c6bc'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'ca2da8fef6b0e723738eb88852f82391', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'edab1575eefd3e4f151dc7010bdf8c13'}\n","{'category_depth': 0, 'emphasized_text_contents': ['API Reference:'], 'emphasized_text_tags': ['b'], 'link_texts': ['AIMessage', 'HumanMessage', 'SystemMessage', 'ChatPromptTemplate', 'MessagesPlaceholder'], 'link_urls': ['https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html', 'https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html', 'https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html', 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html'], 'languages': ['eng'], 'filetype': 'text/html', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': '5d13f62c53f3e352a413103dad3aaed3'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '5d13f62c53f3e352a413103dad3aaed3', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '9e92e938aed1fdde5017435020adabdc'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '5d13f62c53f3e352a413103dad3aaed3', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '13fe2cd9176b27af4b079048064e9b84'}\n","{'category_depth': 1, 'link_texts': ['\\u200b'], 'link_urls': ['#automatic-history-management'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '5d13f62c53f3e352a413103dad3aaed3', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': '4da504ed3da1e1b3dfb973b9e3008ca3'}\n","{'link_texts': ['persistence', 'enable persistence'], 'link_urls': ['https://langchain-ai.github.io/langgraph/concepts/persistence/', 'https://langchain-ai.github.io/langgraph/how-tos/persistence/'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '4da504ed3da1e1b3dfb973b9e3008ca3', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '45ec596a31e48876d9c57d88ecdd7ecd'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '4da504ed3da1e1b3dfb973b9e3008ca3', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'aa4c504f31c76d6b78afc25a1f208fed'}\n","{'category_depth': 0, 'emphasized_text_contents': ['API Reference:'], 'emphasized_text_tags': ['b'], 'link_texts': ['MemorySaver', 'StateGraph'], 'link_urls': ['https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver', 'https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph'], 'languages': ['eng'], 'filetype': 'text/html', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': '64e27e730e0f9e2d0c174b33f9e50d1c'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '64e27e730e0f9e2d0c174b33f9e50d1c', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '95dab31ce9a50eb3520ee60302610fcc'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '64e27e730e0f9e2d0c174b33f9e50d1c', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '6308bcb7062a75b571c839b8e267dd82'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '64e27e730e0f9e2d0c174b33f9e50d1c', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'e79e3d033f3c9e7f9ab78f21d1e32014'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '64e27e730e0f9e2d0c174b33f9e50d1c', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '1ca6c0402da1d1a29e2cae5fc656014a'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '64e27e730e0f9e2d0c174b33f9e50d1c', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '690b3a0dd67d7ebc86798aef550a0153'}\n","{'category_depth': 1, 'link_texts': ['\\u200b'], 'link_urls': ['#modifying-chat-history'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '64e27e730e0f9e2d0c174b33f9e50d1c', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': 'bab55776facb21f43f2fefcbc44b2210'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'bab55776facb21f43f2fefcbc44b2210', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '0d680bfcded72deeb615c065f0dabcde'}\n","{'category_depth': 2, 'link_texts': ['\\u200b'], 'link_urls': ['#trimming-messages'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'bab55776facb21f43f2fefcbc44b2210', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': '6a5e90c0e3d3cf660d949a925ba30091'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '6a5e90c0e3d3cf660d949a925ba30091', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '18088fd8d9e175209ac1a55ace6e3bbc'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '6a5e90c0e3d3cf660d949a925ba30091', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'f5b1e2f882d5dc02df0db5311aa7ee6e'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '6a5e90c0e3d3cf660d949a925ba30091', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'b61739e07de1ba151bfc17151bd4dd9f'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '6a5e90c0e3d3cf660d949a925ba30091', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '1623d8a8a06835caef6493b065f3362e'}\n","{'link_texts': ['trim_messages'], 'link_urls': ['/docs/how_to/trim_messages/'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '6a5e90c0e3d3cf660d949a925ba30091', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '494dbe877c6b5db27ba71b36fe3dd1ab'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '6a5e90c0e3d3cf660d949a925ba30091', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '7420d5cb542fbb510d34d81d8a07694d'}\n","{'emphasized_text_contents': ['API Reference:'], 'emphasized_text_tags': ['b'], 'link_texts': ['trim_messages', 'MemorySaver', 'StateGraph'], 'link_urls': ['https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html', 'https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver', 'https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '6a5e90c0e3d3cf660d949a925ba30091', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '8bfcd9fc845a26333f9df482e3c1aa99'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '6a5e90c0e3d3cf660d949a925ba30091', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'ba7f6acc8411dd72aa52d01ff377d91f'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '6a5e90c0e3d3cf660d949a925ba30091', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'afa08cddf0445c285fe42754db3f7f01'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '6a5e90c0e3d3cf660d949a925ba30091', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '1e191e9011408b17e681e2cccc7ce7eb'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '6a5e90c0e3d3cf660d949a925ba30091', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'a4a6f1a8a066cbad0daa0f0e9bee7e18'}\n","{'link_texts': ['how to guide on trimming messages'], 'link_urls': ['/docs/how_to/trim_messages/'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '6a5e90c0e3d3cf660d949a925ba30091', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '2428f0d4092b08c6f5c36228d3249ce4'}\n","{'category_depth': 2, 'link_texts': ['\\u200b'], 'link_urls': ['#summary-memory'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'bab55776facb21f43f2fefcbc44b2210', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': '20bcb3c50b05e09880565dfa9e801fe0'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '20bcb3c50b05e09880565dfa9e801fe0', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'e52e3571955ea85bb4b7b7df420aa051'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '20bcb3c50b05e09880565dfa9e801fe0', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '1865898a17fb563a875118670a7b34b8'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '20bcb3c50b05e09880565dfa9e801fe0', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '18d6ad575b192ecc9955d7cae802d5de'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '20bcb3c50b05e09880565dfa9e801fe0', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '80d81a6a2efea7f142a589c264d66289'}\n","{'category_depth': 0, 'emphasized_text_contents': ['API Reference:'], 'emphasized_text_tags': ['b'], 'link_texts': ['HumanMessage', 'RemoveMessage', 'MemorySaver', 'StateGraph'], 'link_urls': ['https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html', 'https://python.langchain.com/api_reference/core/messages/langchain_core.messages.modifier.RemoveMessage.html', 'https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver', 'https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph'], 'languages': ['eng'], 'filetype': 'text/html', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': '4f909d03bdb333f1b31cccb1342e03f9'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '4f909d03bdb333f1b31cccb1342e03f9', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': 'ec1c13c2c71fcd0554e06832f6469d36'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '4f909d03bdb333f1b31cccb1342e03f9', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '54573ac471c330f104ccd7e4dd5e92c4'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '4f909d03bdb333f1b31cccb1342e03f9', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '957e5cd9ae145ebfae127bba3b10b02f'}\n","{'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '4f909d03bdb333f1b31cccb1342e03f9', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'NarrativeText', 'element_id': '401e99860877a73dbef8d7c1d95dc9d2'}\n","{'category_depth': 0, 'link_texts': ['Edit this page'], 'link_urls': ['https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/chatbots_memory.ipynb'], 'languages': ['eng'], 'filetype': 'text/html', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': 'adfe1fd3f8ff42982e04b5b3d63f0221'}\n","{'category_depth': 3, 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'adfe1fd3f8ff42982e04b5b3d63f0221', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'Title', 'element_id': 'c09eab23c2580552bf1fa0158d60cc17'}\n","{'category_depth': 1, 'link_texts': ['Setup'], 'link_urls': ['#setup'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'c09eab23c2580552bf1fa0158d60cc17', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'ListItem', 'element_id': '6ac5955243fd4af6125c30da8e8fbf8d'}\n","{'category_depth': 1, 'link_texts': ['Message passing'], 'link_urls': ['#message-passing'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'c09eab23c2580552bf1fa0158d60cc17', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'ListItem', 'element_id': 'e7a57cad13a465e7c98e7395f07e6673'}\n","{'category_depth': 1, 'link_texts': ['Automatic history management'], 'link_urls': ['#automatic-history-management'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'c09eab23c2580552bf1fa0158d60cc17', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'ListItem', 'element_id': 'd226371f724a55c46e033e3c18169354'}\n","{'category_depth': 1, 'link_texts': ['Modifying chat history'], 'link_urls': ['#modifying-chat-history'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': 'c09eab23c2580552bf1fa0158d60cc17', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'ListItem', 'element_id': '0659572e0bdae1332e6656a29c6f34d2'}\n","{'category_depth': 2, 'link_texts': ['Trimming messages'], 'link_urls': ['#trimming-messages'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '0659572e0bdae1332e6656a29c6f34d2', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'ListItem', 'element_id': '79a7d5465858600b591bc7c822e3e9ea'}\n","{'category_depth': 2, 'link_texts': ['Summary memory'], 'link_urls': ['#summary-memory'], 'languages': ['eng'], 'filetype': 'text/html', 'parent_id': '0659572e0bdae1332e6656a29c6f34d2', 'url': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'category': 'ListItem', 'element_id': '7227ca949fbabcc58042021df0a88348'}\n"]}],"source":["for doc in docs:\n","    print(doc.metadata)"]},{"cell_type":"markdown","metadata":{},"source":["Elements may also have parent-child relationships -- \n","\n","for example, a paragraph might belong to a section with a title. \n","\n","If a section is of particular interest (e.g., for indexing) we can isolate the corresponding ```Document``` objects.\n","\n","As an example, below we load the content of the \"Setup\" sections for two web pages:"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from typing import List\n","\n","from langchain_core.documents import Document\n","\n","\n","async def _get_setup_docs_from_url(url: str) -> List[Document]:\n","    loader = UnstructuredLoader(web_url=url)\n","\n","    setup_docs = []\n","    parent_id = -1\n","    async for doc in loader.alazy_load():\n","        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):\n","            parent_id = doc.metadata[\"element_id\"]\n","        if doc.metadata.get(\"parent_id\") == parent_id:\n","            setup_docs.append(doc)\n","\n","    return setup_docs\n","\n","\n","page_urls = [\n","    \"https://python.langchain.com/docs/how_to/chatbots_memory/\",\n","    \"https://python.langchain.com/docs/how_to/chatbots_tools/\",\n","]\n","setup_docs = []\n","for url in page_urls:\n","    page_setup_docs = await _get_setup_docs_from_url(url)\n","    setup_docs.extend(page_setup_docs)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["{'https://python.langchain.com/docs/how_to/chatbots_memory/': 'You\\'ll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\\n%pip install --upgrade --quiet langchain langchain-openai langgraph\\n\\nimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\n',\n"," 'https://python.langchain.com/docs/how_to/chatbots_tools/': 'For this guide, we\\'ll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you\\'re using Tavily.\\nYou\\'ll need to sign up for an account on the Tavily website, and install the following packages:\\n%pip install --upgrade --quiet langchain-community langchain-openai tavily-python langgraph\\n\\nimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\n\\nif not os.environ.get(\"TAVILY_API_KEY\"):\\n    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API Key:\")\\n'}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from collections import defaultdict\n","\n","setup_text = defaultdict(str)\n","\n","for doc in setup_docs:\n","    url = doc.metadata[\"url\"]\n","    setup_text[url] += f\"{doc.page_content}\\n\"\n","\n","dict(setup_text)"]},{"cell_type":"markdown","metadata":{},"source":["# Vector search over page content\n","Once we have loaded the page contents into LangChain Document objects, we can index them (e.g., for a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain embeddings model will suffice."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n","INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"]},{"name":"stdout","output_type":"stream","text":["Page https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n","\n","Page https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n","\n"]}],"source":["from langchain_core.vectorstores import InMemoryVectorStore\n","from langchain_openai import OpenAIEmbeddings\n","\n","vector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\n","retrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\n","for doc in retrieved_docs:\n","    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":2}
